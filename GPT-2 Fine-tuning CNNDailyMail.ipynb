{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning GPT-2 for text summarisation using Hugging Face *Transformers*\n",
        "\n",
        "source: https://github.com/jwhogg"
      ],
      "metadata": {
        "id": "WBDp_4ZxkIKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check we have GPU to train on:"
      ],
      "metadata": {
        "id": "eZoCH8aJjVpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "wIhyYyyWjWT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Dataset:\n",
        "- we will be using the CNN/DailyMail Dataset, which has articles, and their corresponding summaries\n",
        "- `raw_dataset` is a DataSet Dict with 'train'/'validation'/'test' split\n",
        "- for CNN/DailyMail, version must be specified, as it has V1-3\n",
        "```\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['article', 'highlights', 'id'],\n",
        "        num_rows: 287113\n",
        "    })\n",
        "    validation: Dataset({\n",
        "        features: ['article', 'highlights', 'id'],\n",
        "        num_rows: 13368\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ['article', 'highlights', 'id'],\n",
        "        num_rows: 11490\n",
        "    })\n",
        "})\n",
        "```"
      ],
      "metadata": {
        "id": "zvBXQUECjaAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets\n",
        "# from datasets import load_dataset\n",
        "# raw_datasets = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "QJVLnyycjdLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the model tokeniser:"
      ],
      "metadata": {
        "id": "alK8Obq_jgrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "if tokenizer.pad_token is None: #assigning a value to the pad token so we can pad up to gpt2's input length\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ],
      "metadata": {
        "id": "lACFw8SZjiGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create encodings for train/test using tokeniser:\n",
        "- 'map()' lets us run the tokenizer function on the train/valid/test dicts individually, tokenizing each column ('article' & 'highlights')"
      ],
      "metadata": {
        "id": "YODcGELHjj1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "S1XPwIKbjlXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create small datasets for development"
      ],
      "metadata": {
        "id": "ocfRA3wUjomt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "full_train_dataset = tokenized_datasets[\"train\"]\n",
        "full_eval_dataset = tokenized_datasets[\"test\"]"
      ],
      "metadata": {
        "id": "UUss-FPUjqUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Model"
      ],
      "metadata": {
        "id": "ielpdr3MjsWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "\n",
        "model = GPT2Model.from_pretrained('gpt2-medium').to(device)"
      ],
      "metadata": {
        "id": "R1xafbERjt4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "Transformers has a Trainer class that can speed up training of models, and does a lot of the work for us\n",
        "Trainer is defined as a dict of arguments and a compute_metrics function, but first we need to define these:\n",
        "Training args:\n",
        "- use just default args to start with\n",
        "- add arg: evaluation_strategy=\"epoch\" to report metrics every epoch"
      ],
      "metadata": {
        "id": "9sHOtMpzjvfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "#if the code throws an 'accelerate'-related error, try to re-install transformers with relevant torch dependencies, then restart the notebook\n",
        "#!pip install transformers[torch]\n",
        "training_args = TrainingArguments(\"test_trainer\")"
      ],
      "metadata": {
        "id": "21XLH7yijxSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure training metrics\n",
        "Trainer can take a `compute_metrics()` function, which takes predictions and labels (in a tuple), and returns a dict with metric names and values\n",
        "we can use the Datasets library to get access to common metrics- 'accuracy' is one of these"
      ],
      "metadata": {
        "id": "3DcRDXT8jzZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "GQod4m9Nj1AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Trainer"
      ],
      "metadata": {
        "id": "rHdVnlpLj27H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "_tQEWytWj4DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate"
      ],
      "metadata": {
        "id": "42xBj4rOj5us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "5h5PY1Kxj8N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We* are now done! the [training args](https://huggingface.co/docs/transformers/v4.15.0/en/main_classes/trainer#transformers.TrainingArguments) or dataset can be tweaked to try to improve performance"
      ],
      "metadata": {
        "id": "XTs8rIuej-D0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remember to save your model!**\n",
        "```python\n",
        "model.save_pretrained(\"path/to/model.pt\")\n",
        "```\n",
        "- for google colab, you will need to download the model to your local machine, as the colab files are wiped clean when the runtime ends"
      ],
      "metadata": {
        "id": "tIRlX_pIkBIu"
      }
    }
  ]
}